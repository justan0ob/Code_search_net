{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d30111f2",
   "metadata": {},
   "source": [
    "# Knowledge Ditillation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6bc0be",
   "metadata": {},
   "source": [
    "# Importing all the neccesary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0450653a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/msaqib/anaconda3/lib/python3.11/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/Users/msaqib/anaconda3/lib/python3.11/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import os,sys\n",
    "sys.path.append('../')\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoModel\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from src.model_evaluation import evaluation\n",
    "from src.other_function import functions\n",
    "from src.distillation_class import KnowledgeDistillation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b73c89",
   "metadata": {},
   "source": [
    "# Creating the object of evaluation class and other function class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00cb44bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_object=evaluation()\n",
    "\n",
    "func_object=functions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff49dcf",
   "metadata": {},
   "source": [
    "# loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "759803f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"..//Data/processed_data.csv\")\n",
    "\n",
    "\n",
    "list_data = df['tokenized_docstring'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1363518",
   "metadata": {},
   "source": [
    "# Loading the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e0349cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries=pd.read_csv(\"..//Data/query.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733a67c1",
   "metadata": {},
   "source": [
    "# Loading the teacher model and student model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "380e17b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model=SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "student_model=SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73c24b3",
   "metadata": {},
   "source": [
    "# Architecture of the student Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02820e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(30522, 384, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 384)\n",
      "    (token_type_embeddings): Embedding(2, 384)\n",
      "    (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "student_model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L12-v2')\n",
    "\n",
    "print(student_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef2c441",
   "metadata": {},
   "source": [
    "# Creating the object of knowlegde distillation class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74ffd606",
   "metadata": {},
   "outputs": [],
   "source": [
    "know_dist_object=KnowledgeDistillation(\"all-mpnet-base-v2\",\"all-MiniLM-L12-v2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def8814d",
   "metadata": {},
   "source": [
    "# Training the student model on the teacher model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "496eb29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "know_dist_object.train_student_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33191e06",
   "metadata": {},
   "source": [
    "# Loading the model from local drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "49db8f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "distillated_model=SentenceTransformer('..//models/distiiled_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab1cf9a",
   "metadata": {},
   "source": [
    "# Architecture of distillated model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a993701d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(30522, 384, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 384)\n",
      "    (token_type_embeddings): Embedding(2, 384)\n",
      "    (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_path = \"..//models/distiiled_model\"\n",
    "distillated_model_arch = AutoModel.from_pretrained(model_path)\n",
    "\n",
    "# Print the model architecture\n",
    "print(distillated_model_arch)\n",
    "\n",
    "# Our distillate model has 12 layers with the vector dimension size of 384 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0529936",
   "metadata": {},
   "source": [
    "# generating the embeddings of distillated model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "95ecf918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting_time : 1713431072.99\n",
      "ending_time : 1713431400.05\n",
      "Embedding_shape : (113884, 384)\n",
      "total_time_taken for embedding generation:  327.06 seconds \n"
     ]
    }
   ],
   "source": [
    "func_object.get_model_embedding(distillated_model,\"distilled_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959433fb",
   "metadata": {},
   "source": [
    "# size of distillated model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5309b205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of 6 layer model is 148.20 MB\n"
     ]
    }
   ],
   "source": [
    "size=func_object.get_model_size(\"..//models/distilled_model_with_6_layer\")\n",
    "\n",
    "print(f\"Size of 6 layer model is {(size/(1024*1024)):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa06d206",
   "metadata": {},
   "source": [
    "# size of embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b5c227a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embeddings of 6 layer model is 333.64 MB\n"
     ]
    }
   ],
   "source": [
    "size_of_embeddings=func_object.get_embedding_size(\"..//embeddings/embeddings_distilled_model_with_6_layer.npy\")\n",
    "\n",
    "print(f\"Size of embeddings of 6 layer model is {(size_of_embeddings/(1024*1024)):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8f6294",
   "metadata": {},
   "source": [
    "# Response on the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8718f3b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(113884, 384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# order in which column of databse will be present\n",
    "column_order = ['Query', 'Docstrings', 'Code','Match']\n",
    "\n",
    "#creating the dataframe for model_1 which consists of for particular query all top code and\n",
    "#match column shows whether it is correctly retrieved or not\n",
    "dist_model_query_response=pd.DataFrame(columns=column_order)\n",
    "\n",
    "distillated_model=SentenceTransformer('..//models/distiiled_model')\n",
    "\n",
    "emb_dist_model=np.load(\"..//embeddings/embeddings_distilled_model.npy\")\n",
    "\n",
    "print(emb_dist_model.shape)\n",
    "\n",
    "for i in range(len(queries)):\n",
    "    #getting the particular query from queries \n",
    "    query=queries.loc[i].Questions\n",
    "    \n",
    "    # getting the table dataframe that consists of docstring and code \n",
    "    #that has top match with query using cosine similarity\n",
    "    table=eval_object.get_top_code_and_docstring(query,emb_dist_model,distillated_model,list_data,df)\n",
    "    \n",
    "    #adding the column query with all values as \n",
    "    table=add_column(table,query)\n",
    "    \n",
    "    #adding columns Match that defines whether the code for given query is correctly retrieved or not\n",
    "    table=add_column(table,'Match')\n",
    "    \n",
    "    #reordering the table for better view\n",
    "    table=reorder_columns(table)\n",
    "    \n",
    "    #iterating in the table\n",
    "    for index,row in table.iterrows():\n",
    "        \n",
    "        #fetching the current code for query\n",
    "        result = row['Code']\n",
    "        \n",
    "        #getting the response from claude api\n",
    "        response=eval_object.check_response(query,row['Code'])\n",
    "    \n",
    "        #Checking the value present in the response generated by the claude\n",
    "        if 'YES' in response.content:\n",
    "            response='YES'\n",
    "        else :\n",
    "            response='NO'\n",
    "            \n",
    "        #Giving the value of response in match column for current row\n",
    "        table.at[index,'Match']=response\n",
    "    \n",
    "    #merging the table dataframe and model_1_query_response in single dataframe\n",
    "    dist_model_query_response= pd.concat([dist_model_query_response, table], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cb8fbc",
   "metadata": {},
   "source": [
    "# MAP@10 of distilled model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2ae3a88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@10 (mean average precision) of 8 layer model : 62.63157894736842\n"
     ]
    }
   ],
   "source": [
    "total_yes = (dist_model_query_response['Match'] == 'YES').sum()\n",
    "total_no = (dist_model_query_response['Match'] == 'NO').sum()\n",
    "\n",
    "accuracy_dist_model=(total_yes/(total_yes+total_no))*100\n",
    "\n",
    "print(f\"MAP@10 (mean average precision) of 8 layer model : {accuracy_dist_model}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e241fed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bb2312",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
